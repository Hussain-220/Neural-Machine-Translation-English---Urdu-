{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnsHC767o6PK"
      },
      "source": [
        "# Neural Machine Translation: English → Urdu\n",
        "## DS-462 Gen AI with LLM \n",
        "\n",
        "**Student ID:** 2022220  \n",
        "**Date:** 19 October 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Assignment Overview\n",
        "\n",
        "This notebook implements and compares **four machine translation models**:\n",
        "1. **RNN Encoder-Decoder**\n",
        "2. **LSTM Encoder-Decoder**\n",
        "3. **GRU Encoder-Decoder**\n",
        "4. **Transformer (Attention-based)**\n",
        "\n",
        "### Dataset\n",
        "- **Source:** English-Urdu Parallel Corpus\n",
        "- **Size:** 50,000 sentence pairs\n",
        "- **Task:** English → Urdu Translation\n",
        "\n",
        "### Evaluation Metrics\n",
        "✓ BLEU Score  \n",
        "✓ CHRF Score  \n",
        "✓ Perplexity  \n",
        "✓ METEOR  \n",
        "✓ BERTScore\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq75XlNdn7Eo",
        "outputId": "940096d2-6a29-46dd-a332-794a3158dbf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "✓ All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk bert-score sacrebleu -q\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Standard libraries\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, GRU, Dropout\n",
        "from tensorflow.keras.layers import SimpleRNN, Bidirectional, Attention, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Evaluation metrics\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Checking GPU availability\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('GPU Available:', tf.config.list_physical_devices('GPU'))\n",
        "print('\\n✓ All libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPhYEz9OpJEy"
      },
      "source": [
        "### Global Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ4zPkjKoH2S",
        "outputId": "3f6c2373-ec90-4585-e9da-41207a8cf52b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Samples: 50,000\n",
            "  Vocabulary Size: 15,000\n",
            "  Max Length: 40\n",
            "  Embedding Dim: 128\n",
            "  Hidden Units: 128\n",
            "  Batch Size: 64\n",
            "  Epochs: 15\n"
          ]
        }
      ],
      "source": [
        "# Global Configuration for all models\n",
        "class Config:\n",
        "    # Data parameters\n",
        "    NUM_SAMPLES = 50000        # Use 50,000 sentence pairs\n",
        "    MAX_VOCAB_SIZE = 15000     # Vocabulary size (Colab-friendly---originally it was about 50,000)\n",
        "    MAX_LENGTH = 40            # Maximum sentence length\n",
        "\n",
        "    # Model parameters\n",
        "    EMBEDDING_DIM = 128        # Embedding dimension\n",
        "    HIDDEN_UNITS = 128         # RNN/LSTM/GRU hidden units\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 64            # Batch size\n",
        "    EPOCHS = 15                # Training epochs\n",
        "    VALIDATION_SPLIT = 0.1     # Validation split\n",
        "    TEST_SPLIT = 0.1           # Test split\n",
        "\n",
        "    # Paths\n",
        "    DATA_DIR = Path('.')\n",
        "    RESULTS_DIR = Path('results')\n",
        "    MODELS_DIR = Path('models')\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create directories\n",
        "        self.RESULTS_DIR.mkdir(exist_ok=True)\n",
        "        self.MODELS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print('Configuration:')\n",
        "print(f'  Samples: {config.NUM_SAMPLES:,}')\n",
        "print(f'  Vocabulary Size: {config.MAX_VOCAB_SIZE:,}')\n",
        "print(f'  Max Length: {config.MAX_LENGTH}')\n",
        "print(f'  Embedding Dim: {config.EMBEDDING_DIM}')\n",
        "print(f'  Hidden Units: {config.HIDDEN_UNITS}')\n",
        "print(f'  Batch Size: {config.BATCH_SIZE}')\n",
        "print(f'  Epochs: {config.EPOCHS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eDg28z0pMXd"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Dataset Selection & Justification\n",
        "\n",
        "**Dataset Name:** English-Urdu Parallel Corpus  \n",
        "**Dataset Source:**  \n",
        "[Urdu–English Machine Translation Dataset (GitHub)](https://github.com/Kheem-Dh/Urdu-to-English-Machine-Translation-using-Seq2Seq-Transformers-Variant-Model)\n",
        "**Languages:** English (source) → Urdu (target)  \n",
        "**Total Sentence Pairs:** 50,000 (selected from 100,000)  \n",
        "\n",
        "### Why This Dataset is Challenging\n",
        "\n",
        "1. **Script Difference:** English uses Latin script while Urdu uses Arabic script (right-to-left)\n",
        "2. **Grammar Structure:** Urdu follows Subject-Object-Verb (SOV) order vs English Subject-Verb-Object (SVO)\n",
        "3. **Morphological Complexity:** Urdu has rich inflections and compound words\n",
        "4. **Context-Dependent Translation:** Same words translate differently based on context\n",
        "5. **Low-Resource Language:** Limited parallel data compared to high-resource language pairs\n",
        "6. **Informal Expressions:** Dataset contains colloquial language and idiomatic expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YmOmt21pReC"
      },
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "### Step 1: Load and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahA1qA3ioLNd",
        "outputId": "e3c1633c-9dee-4e0a-ea79-4ae9a86d2e65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading English-Urdu parallel corpus...\n",
            "Total sentences loaded: 100,000\n",
            "Cleaned sentences: 50,000\n",
            "\n",
            "Dataset Statistics:\n",
            "  Average English length: 12.41 words\n",
            "  Average Urdu length: 15.83 words\n",
            "\n",
            "Sample Data:\n",
            "1. EN: los angeles has lost night straight and  of its first  games to start the season\n",
            "   UR: لاس اینجلس نے سیزن شروع کرنے کے لئے سیدھے رات اور اپنے پہلے  میں سے  کھیل کھوئے ہیں۔\n",
            "\n",
            "2. EN: opposite qualities of meaning of persons name\n",
            "   UR: آنکھ کا اندھا نام نین سکھ\n",
            "\n",
            "3. EN: to show anger after getting embarrassed\n",
            "   UR: کھسیانی بلی کھمبا نوچے\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean text: lowercase, remove punctuation/numbers, strip whitespace\"\"\"\n",
        "    text = text.lower()\n",
        "    # Keep Urdu unicode range and English letters\n",
        "    text = re.sub(r'[^\\u0600-\\u06FFa-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Load data\n",
        "print('Loading English-Urdu parallel corpus...')\n",
        "with open('English.txt', 'r', encoding='utf-8-sig', errors='ignore') as f:\n",
        "    english_sentences = f.read().strip().split('\\n')\n",
        "\n",
        "with open('Urdu.txt', 'r', encoding='utf-8-sig', errors='ignore') as f:\n",
        "    urdu_sentences = f.read().strip().split('\\n')\n",
        "\n",
        "print(f'Total sentences loaded: {len(english_sentences):,}')\n",
        "\n",
        "# Clean and filter data\n",
        "eng_clean = []\n",
        "urdu_clean = []\n",
        "\n",
        "for eng, urdu in zip(english_sentences, urdu_sentences):\n",
        "    eng_c = clean_text(eng)\n",
        "    urdu_c = clean_text(urdu)\n",
        "\n",
        "    # Filter: non-empty, reasonable length\n",
        "    if (eng_c and urdu_c and\n",
        "        len(eng_c.split()) <= config.MAX_LENGTH and\n",
        "        len(urdu_c.split()) <= config.MAX_LENGTH and\n",
        "        len(eng_c.split()) >= 2 and\n",
        "        len(urdu_c.split()) >= 2):\n",
        "        eng_clean.append(eng_c)\n",
        "        urdu_clean.append(urdu_c)\n",
        "\n",
        "    # Stop at desired number\n",
        "    if len(eng_clean) >= config.NUM_SAMPLES:\n",
        "        break\n",
        "\n",
        "print(f'Cleaned sentences: {len(eng_clean):,}')\n",
        "\n",
        "# Compute statistics\n",
        "avg_eng_len = np.mean([len(s.split()) for s in eng_clean])\n",
        "avg_urdu_len = np.mean([len(s.split()) for s in urdu_clean])\n",
        "\n",
        "print(f'\\nDataset Statistics:')\n",
        "print(f'  Average English length: {avg_eng_len:.2f} words')\n",
        "print(f'  Average Urdu length: {avg_urdu_len:.2f} words')\n",
        "\n",
        "# Display samples\n",
        "print(f'\\nSample Data:')\n",
        "for i in range(3):\n",
        "    print(f'{i+1}. EN: {eng_clean[i]}')\n",
        "    print(f'   UR: {urdu_clean[i]}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li5ASVXLpXDE"
      },
      "source": [
        "### Step 2: Prepare Sequences with Start/End Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZAFLEupoR37",
        "outputId": "d9b6eb7d-e755-4d28-93b0-f89b2fdf3039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder sequence examples:\n",
            "Input:  <start> لاس اینجلس نے سیزن شروع کرنے کے لئے سیدھے رات اور اپنے پہلے  میں سے  کھیل کھوئے ہیں۔\n",
            "Target: لاس اینجلس نے سیزن شروع کرنے کے لئے سیدھے رات اور اپنے پہلے  میں سے  کھیل کھوئے ہیں۔ <end>\n"
          ]
        }
      ],
      "source": [
        "# Prepare decoder sequences with special tokens\n",
        "decoder_input_data = ['<start> ' + s for s in urdu_clean]\n",
        "decoder_target_data = [s + ' <end>' for s in urdu_clean]\n",
        "\n",
        "print('Decoder sequence examples:')\n",
        "print(f'Input:  {decoder_input_data[0]}')\n",
        "print(f'Target: {decoder_target_data[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcutNOa1paUd"
      },
      "source": [
        "### Step 3: Tokenization and Vocabulary Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG2o-Q_Zoam9",
        "outputId": "5bf5e7bc-0771-423c-e11d-0023676bceaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English vocabulary size: 15,000\n",
            "Urdu vocabulary size: 15,000\n",
            "\n",
            "✓ Tokenizers saved\n"
          ]
        }
      ],
      "source": [
        "# Tokenize English (source)\n",
        "eng_tokenizer = Tokenizer(num_words=config.MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "eng_tokenizer.fit_on_texts(eng_clean)\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(eng_clean)\n",
        "\n",
        "eng_vocab_size = min(len(eng_tokenizer.word_index) + 1, config.MAX_VOCAB_SIZE)\n",
        "print(f'English vocabulary size: {eng_vocab_size:,}')\n",
        "\n",
        "# Tokenize Urdu (target)\n",
        "urdu_tokenizer = Tokenizer(num_words=config.MAX_VOCAB_SIZE, filters='', oov_token='<OOV>')\n",
        "urdu_tokenizer.fit_on_texts(decoder_input_data + decoder_target_data)\n",
        "\n",
        "decoder_input_sequences = urdu_tokenizer.texts_to_sequences(decoder_input_data)\n",
        "decoder_target_sequences = urdu_tokenizer.texts_to_sequences(decoder_target_data)\n",
        "\n",
        "urdu_vocab_size = min(len(urdu_tokenizer.word_index) + 1, config.MAX_VOCAB_SIZE)\n",
        "print(f'Urdu vocabulary size: {urdu_vocab_size:,}')\n",
        "\n",
        "# Save tokenizers\n",
        "with open(config.MODELS_DIR / 'eng_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(eng_tokenizer, f)\n",
        "with open(config.MODELS_DIR / 'urdu_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(urdu_tokenizer, f)\n",
        "\n",
        "print('\\n✓ Tokenizers saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTE4fFW2peN8"
      },
      "source": [
        "### Step 4: Padding and Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axnz0dxdojA1",
        "outputId": "724cef1c-b91c-4b40-df8b-933edae00db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max English length: 26\n",
            "Max Urdu length: 41\n",
            "\n",
            "Padded shapes:\n",
            "  Encoder input: (50000, 26)\n",
            "  Decoder input: (50000, 41)\n",
            "  Decoder target: (50000, 41)\n",
            "\n",
            "Data splits:\n",
            "  Train: 40,000 samples (80.0%)\n",
            "  Val:   5,000 samples (10.0%)\n",
            "  Test:  5,000 samples (10.0%)\n"
          ]
        }
      ],
      "source": [
        "# Determine max lengths\n",
        "max_eng_len = min(config.MAX_LENGTH, max(len(s) for s in eng_sequences))\n",
        "max_urdu_len = min(config.MAX_LENGTH + 1, max(len(s) for s in decoder_target_sequences))\n",
        "\n",
        "print(f'Max English length: {max_eng_len}')\n",
        "print(f'Max Urdu length: {max_urdu_len}')\n",
        "\n",
        "# Pad sequences\n",
        "encoder_input_data = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
        "decoder_input_data_padded = pad_sequences(decoder_input_sequences, maxlen=max_urdu_len, padding='post')\n",
        "decoder_target_data_padded = pad_sequences(decoder_target_sequences, maxlen=max_urdu_len, padding='post')\n",
        "\n",
        "print(f'\\nPadded shapes:')\n",
        "print(f'  Encoder input: {encoder_input_data.shape}')\n",
        "print(f'  Decoder input: {decoder_input_data_padded.shape}')\n",
        "print(f'  Decoder target: {decoder_target_data_padded.shape}')\n",
        "\n",
        "# Split data: 80% train, 10% val, 10% test\n",
        "indices = np.arange(len(encoder_input_data))\n",
        "train_idx, test_idx = train_test_split(indices, test_size=config.TEST_SPLIT, random_state=42)\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=config.VALIDATION_SPLIT/(1-config.TEST_SPLIT), random_state=42)\n",
        "\n",
        "X_train = encoder_input_data[train_idx]\n",
        "X_val = encoder_input_data[val_idx]\n",
        "X_test = encoder_input_data[test_idx]\n",
        "\n",
        "y_train_input = decoder_input_data_padded[train_idx]\n",
        "y_val_input = decoder_input_data_padded[val_idx]\n",
        "y_test_input = decoder_input_data_padded[test_idx]\n",
        "\n",
        "y_train_target = decoder_target_data_padded[train_idx]\n",
        "y_val_target = decoder_target_data_padded[val_idx]\n",
        "y_test_target = decoder_target_data_padded[test_idx]\n",
        "\n",
        "print(f'\\nData splits:')\n",
        "print(f'  Train: {len(X_train):,} samples ({100*len(X_train)/len(encoder_input_data):.1f}%)')\n",
        "print(f'  Val:   {len(X_val):,} samples ({100*len(X_val)/len(encoder_input_data):.1f}%)')\n",
        "print(f'  Test:  {len(X_test):,} samples ({100*len(X_test)/len(encoder_input_data):.1f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49WF0FVph6q"
      },
      "source": [
        "## 3. Data Representations (Word Embeddings)\n",
        "\n",
        "We use trainable word embeddings (Keras Embedding layers) for all models. These embeddings are learned during training and capture semantic relationships between words.\n",
        "\n",
        "**Configuration:**\n",
        "- Embedding dimension: 128\n",
        "- Separate embeddings for English (encoder) and Urdu (decoder)\n",
        "- Mask zero enabled for padding tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl_jx0gipkp7"
      },
      "source": [
        "---\n",
        "# Model Implementations\n",
        "\n",
        "We implement 4 different sequence-to-sequence architectures:\n",
        "1. **RNN Encoder-Decoder**\n",
        "2. **LSTM Encoder-Decoder**\n",
        "3. **GRU Encoder-Decoder**\n",
        "4. **Transformer with Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "TZTWMsQOomka",
        "outputId": "719a3ce2-8390-415e-a2a2-2c5a471f5950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN Model Architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_rnn1        │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_rnn2        │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ encoder_rnn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_rnn1        │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_rnn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_rnn2        │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ decoder_rnn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_rnn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,935,000</span> │ decoder_rnn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,920,000\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_rnn1        │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m32,896\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mSimpleRNN\u001b[0m)         │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,920,000\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_rnn2        │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m32,896\u001b[0m │ encoder_rnn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mSimpleRNN\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_rnn1        │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m32,896\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mSimpleRNN\u001b[0m)         │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_rnn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_rnn2        │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m32,896\u001b[0m │ decoder_rnn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mSimpleRNN\u001b[0m)         │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_rnn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m1,935,000\u001b[0m │ decoder_rnn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m15000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,906,584</span> (22.53 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,906,584\u001b[0m (22.53 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,906,584</span> (22.53 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,906,584\u001b[0m (22.53 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# RNN Encoder-Decoder Model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
        "encoder_embedding = Embedding(Config.MAX_VOCAB_SIZE, Config.EMBEDDING_DIM, mask_zero=True)(encoder_inputs)\n",
        "encoder_rnn1 = SimpleRNN(Config.HIDDEN_UNITS, return_sequences=True, return_state=True, name='encoder_rnn1')\n",
        "encoder_rnn2 = SimpleRNN(Config.HIDDEN_UNITS, return_state=True, name='encoder_rnn2')\n",
        "\n",
        "encoder_outputs1, state1 = encoder_rnn1(encoder_embedding)\n",
        "encoder_outputs2, encoder_state = encoder_rnn2(encoder_outputs1)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
        "decoder_embedding = Embedding(Config.MAX_VOCAB_SIZE, Config.EMBEDDING_DIM, mask_zero=True)(decoder_inputs)\n",
        "decoder_rnn1 = SimpleRNN(Config.HIDDEN_UNITS, return_sequences=True, return_state=True, name='decoder_rnn1')\n",
        "decoder_rnn2 = SimpleRNN(Config.HIDDEN_UNITS, return_sequences=True, return_state=True, name='decoder_rnn2')\n",
        "\n",
        "decoder_outputs1, _ = decoder_rnn1(decoder_embedding, initial_state=encoder_state)\n",
        "decoder_outputs2, _ = decoder_rnn2(decoder_outputs1, initial_state=encoder_state)\n",
        "\n",
        "decoder_dense = Dense(Config.MAX_VOCAB_SIZE, activation='softmax', name='output_layer')\n",
        "decoder_outputs = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Define the model\n",
        "rnn_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"RNN Model Architecture:\")\n",
        "rnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkxgKSuiov5G",
        "outputId": "97c5aff9-58a0-4847-b64d-954f320084d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RNN Model...\n",
            "Epoch 1/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - accuracy: 0.0268 - loss: 6.8010 - val_accuracy: 0.0614 - val_loss: 5.6325\n",
            "Epoch 2/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.0711 - loss: 5.4280 - val_accuracy: 0.0841 - val_loss: 5.1594\n",
            "Epoch 3/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.0896 - loss: 4.9784 - val_accuracy: 0.0908 - val_loss: 4.9929\n",
            "Epoch 4/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.0980 - loss: 4.7535 - val_accuracy: 0.0958 - val_loss: 4.8755\n",
            "Epoch 5/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.1037 - loss: 4.5855 - val_accuracy: 0.0983 - val_loss: 4.8110\n",
            "Epoch 6/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.1076 - loss: 4.4538 - val_accuracy: 0.1006 - val_loss: 4.7609\n",
            "Epoch 7/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.1116 - loss: 4.3309 - val_accuracy: 0.1018 - val_loss: 4.7331\n",
            "Epoch 8/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.1153 - loss: 4.2291 - val_accuracy: 0.1020 - val_loss: 4.7315\n",
            "Epoch 9/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.1186 - loss: 4.1398 - val_accuracy: 0.1034 - val_loss: 4.7068\n",
            "Epoch 10/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.1221 - loss: 4.0574 - val_accuracy: 0.1041 - val_loss: 4.7124\n",
            "Epoch 11/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.1241 - loss: 3.9830 - val_accuracy: 0.1040 - val_loss: 4.7268\n",
            "Epoch 12/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.1268 - loss: 3.9237 - val_accuracy: 0.1043 - val_loss: 4.7302\n",
            "\n",
            "RNN Model Training Complete!\n"
          ]
        }
      ],
      "source": [
        "# Train RNN Model\n",
        "print(\"Training RNN Model...\")\n",
        "\n",
        "rnn_checkpoint = ModelCheckpoint('rnn_best_model.keras', save_best_only=True, monitor='val_loss')\n",
        "rnn_early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "rnn_history = rnn_model.fit(\n",
        "    [X_train, y_train_input],\n",
        "    y_train_target,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    epochs=Config.EPOCHS,\n",
        "    validation_data=([X_val, y_val_input], y_val_target),\n",
        "    callbacks=[rnn_checkpoint, rnn_early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nRNN Model Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbAPtvnOpuUs"
      },
      "source": [
        "## 5. Model 2: LSTM Encoder-Decoder\n",
        "\n",
        "**Architecture:**\n",
        "- Encoder: Embedding + 2 LSTM layers\n",
        "- Decoder: Embedding + 2 LSTM layers\n",
        "- Dense output with softmax activation\n",
        "\n",
        "**Advantages over RNN:**\n",
        "- Gating mechanisms (forget, input, output gates)\n",
        "- Better gradient flow for long sequences\n",
        "- Can remember long-term dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "Pl_POZiPo1Zr",
        "outputId": "db559a98-8962-4e89-e3fe-737a321189ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM Model Architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_encoder_input  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> │ lstm_encoder_inp… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_encoder_inp… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_decoder_input  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_encoder_lstm1  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> │ lstm_decoder_inp… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_encoder_lstm2  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ lstm_encoder_lst… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_decoder_lstm1  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_encoder_lst… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_encoder_lst… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_decoder_lstm2  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ lstm_decoder_lst… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_encoder_lst… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_encoder_lst… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_output_layer   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,935,000</span> │ lstm_decoder_lst… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_encoder_input  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,920,000\u001b[0m │ lstm_encoder_inp… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ lstm_encoder_inp… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_decoder_input  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_encoder_lstm1  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m131,584\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mLSTM\u001b[0m)              │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,920,000\u001b[0m │ lstm_decoder_inp… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_encoder_lstm2  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │    \u001b[38;5;34m131,584\u001b[0m │ lstm_encoder_lst… │\n",
              "│ (\u001b[38;5;33mLSTM\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_decoder_lstm1  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m131,584\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mLSTM\u001b[0m)              │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_encoder_lst… │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_encoder_lst… │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_decoder_lstm2  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m131,584\u001b[0m │ lstm_decoder_lst… │\n",
              "│ (\u001b[38;5;33mLSTM\u001b[0m)              │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_encoder_lst… │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_encoder_lst… │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_output_layer   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m1,935,000\u001b[0m │ lstm_decoder_lst… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m15000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,301,336</span> (24.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,301,336\u001b[0m (24.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,301,336</span> (24.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,301,336\u001b[0m (24.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# LSTM Encoder-Decoder Model\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Encoder\n",
        "lstm_encoder_inputs = Input(shape=(None,), name='lstm_encoder_input')\n",
        "lstm_encoder_embedding = Embedding(Config.MAX_VOCAB_SIZE, Config.EMBEDDING_DIM, mask_zero=True)(lstm_encoder_inputs)\n",
        "lstm_encoder_lstm1 = LSTM(Config.HIDDEN_UNITS, return_sequences=True, return_state=True, name='lstm_encoder_lstm1')\n",
        "lstm_encoder_lstm2 = LSTM(Config.HIDDEN_UNITS, return_state=True, name='lstm_encoder_lstm2')\n",
        "\n",
        "lstm_encoder_outputs1, state_h1, state_c1 = lstm_encoder_lstm1(lstm_encoder_embedding)\n",
        "lstm_encoder_outputs2, state_h2, state_c2 = lstm_encoder_lstm2(lstm_encoder_outputs1)\n",
        "\n",
        "# Decoder\n",
        "lstm_decoder_inputs = Input(shape=(None,), name='lstm_decoder_input')\n",
        "lstm_decoder_embedding = Embedding(Config.MAX_VOCAB_SIZE, Config.EMBEDDING_DIM, mask_zero=True)(lstm_decoder_inputs)\n",
        "lstm_decoder_lstm1 = LSTM(Config.HIDDEN_UNITS, return_sequences=True, return_state=True, name='lstm_decoder_lstm1')\n",
        "lstm_decoder_lstm2 = LSTM(Config.HIDDEN_UNITS, return_sequences=True, return_state=True, name='lstm_decoder_lstm2')\n",
        "\n",
        "lstm_decoder_outputs1, _, _ = lstm_decoder_lstm1(lstm_decoder_embedding, initial_state=[state_h2, state_c2])\n",
        "lstm_decoder_outputs2, _, _ = lstm_decoder_lstm2(lstm_decoder_outputs1, initial_state=[state_h2, state_c2])\n",
        "\n",
        "lstm_decoder_dense = Dense(Config.MAX_VOCAB_SIZE, activation='softmax', name='lstm_output_layer')\n",
        "lstm_decoder_outputs = lstm_decoder_dense(lstm_decoder_outputs2)\n",
        "\n",
        "# Define the model\n",
        "lstm_model = Model([lstm_encoder_inputs, lstm_decoder_inputs], lstm_decoder_outputs)\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"LSTM Model Architecture:\")\n",
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6RLSJDyp1FS",
        "outputId": "67d3d361-4aa0-4be3-f53e-9846d9dcf825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM Model...\n",
            "Epoch 1/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - accuracy: 0.0289 - loss: 6.9469 - val_accuracy: 0.0469 - val_loss: 6.0100\n",
            "Epoch 2/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 88ms/step - accuracy: 0.0521 - loss: 5.8574 - val_accuracy: 0.0628 - val_loss: 5.5811\n",
            "Epoch 3/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 87ms/step - accuracy: 0.0683 - loss: 5.4182 - val_accuracy: 0.0794 - val_loss: 5.2341\n",
            "Epoch 4/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 87ms/step - accuracy: 0.0851 - loss: 5.0441 - val_accuracy: 0.0897 - val_loss: 4.9931\n",
            "Epoch 5/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.0956 - loss: 4.7725 - val_accuracy: 0.0962 - val_loss: 4.8401\n",
            "Epoch 6/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 87ms/step - accuracy: 0.1041 - loss: 4.5662 - val_accuracy: 0.1012 - val_loss: 4.7288\n",
            "Epoch 7/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.1099 - loss: 4.4080 - val_accuracy: 0.1051 - val_loss: 4.6442\n",
            "Epoch 8/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 89ms/step - accuracy: 0.1159 - loss: 4.2642 - val_accuracy: 0.1077 - val_loss: 4.5895\n",
            "Epoch 9/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.1212 - loss: 4.1370 - val_accuracy: 0.1105 - val_loss: 4.5404\n",
            "Epoch 10/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.1255 - loss: 4.0148 - val_accuracy: 0.1134 - val_loss: 4.4989\n",
            "Epoch 11/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.1296 - loss: 3.9051 - val_accuracy: 0.1148 - val_loss: 4.4767\n",
            "Epoch 12/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.1346 - loss: 3.8059 - val_accuracy: 0.1155 - val_loss: 4.4605\n",
            "Epoch 13/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.1383 - loss: 3.7050 - val_accuracy: 0.1165 - val_loss: 4.4525\n",
            "Epoch 14/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.1421 - loss: 3.6174 - val_accuracy: 0.1175 - val_loss: 4.4514\n",
            "Epoch 15/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 86ms/step - accuracy: 0.1456 - loss: 3.5328 - val_accuracy: 0.1180 - val_loss: 4.4577\n",
            "\n",
            "LSTM Model Training Complete!\n"
          ]
        }
      ],
      "source": [
        "# Train LSTM Model\n",
        "print(\"Training LSTM Model...\")\n",
        "\n",
        "lstm_checkpoint = ModelCheckpoint('lstm_best_model.keras', save_best_only=True, monitor='val_loss')\n",
        "lstm_early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "lstm_history = lstm_model.fit(\n",
        "    [X_train, y_train_input],\n",
        "    y_train_target,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    epochs=Config.EPOCHS,\n",
        "    validation_data=([X_val, y_val_input], y_val_target),\n",
        "    callbacks=[lstm_checkpoint, lstm_early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nLSTM Model Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbgnD8BupzM7"
      },
      "source": [
        "## 6. Model 3: GRU Encoder-Decoder\n",
        "\n",
        "**Architecture:**\n",
        "- Encoder: Embedding layer + stacked 2 GRU layers (explicit single-layer GRUs with Dropout(0.1) between for regularization) + packed sequences for efficient masking\n",
        "- Decoder: Embedding layer + stacked 2 GRU layers (initial states passed from corresponding encoder layers) + Dense output layer (logits for CrossEntropyLoss, no softmax in forward)\n",
        "- Overall: PyTorch nn.Module classes (`GRUEncoder` and `GRUDecoder`) mimicking TF functional style with named variables (e.g., `gru_encoder_inputs`, `state_h1`, `encoder_state`)\n",
        "\n",
        "**Why PyTorch?**\n",
        "- Chosen for superior efficiency in handling variable-length sequences via packed sequences, which skip computations on padding and avoid TensorFlow's graph-mode masking errors (e.g., StridedSlice/OperatorNotAllowedInGraphError in stacked GRUs)\n",
        "- Enables faster GPU training without needing `run_eagerly=True` or cuDNN workarounds, providing a better speed-performance trade-off compared to the TF implementation\n",
        "\n",
        "**Advantages:**\n",
        "- Simpler than LSTM (fewer parameters, only 2 gates: update and reset)\n",
        "- Faster training while maintaining performance; explicit dropout between layers prevents overfitting in stacked setup\n",
        "- Native support for variable-length sequences via packing, avoiding TF masking issues for quicker GPU execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8ESmi0kp4jL",
        "outputId": "a41e0678-48ad-40bb-f20a-9af9945db27d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7c31e06ba240>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/while_loop.py\", line 488, in while_loop\n",
            "    loop_vars = body(*loop_vars)  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/while_loop.py\", line 479, in <lambda>\n",
            "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 426, in _step\n",
            "    return (time + 1, output_ta_t) + tuple(new_states)  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 420, in <genexpr>\n",
            "    for ta, out in zip(output_ta_t, flat_output)  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs),\n",
            "==================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU Model Architecture:\n",
            "Encoder parameters: 2,118,144\n",
            "Decoder parameters: 4,053,144\n",
            "Total parameters: 6,171,288\n",
            "\n",
            "✓ Note: Stacked GRU model with dropout between layers (PyTorch-style); uses packed sequences for masking in graph mode\n"
          ]
        }
      ],
      "source": [
        "# GRU Encoder-Decoder Model (Stacked Layers with Dropout between) - PyTorch Version\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "\n",
        "# Config assumptions (adapt if different)\n",
        "SRC_VOCAB_SIZE = Config.MAX_VOCAB_SIZE\n",
        "TGT_VOCAB_SIZE = Config.MAX_VOCAB_SIZE\n",
        "EMB_SIZE = Config.EMBEDDING_DIM\n",
        "HIDDEN_SIZE = Config.HIDDEN_UNITS\n",
        "DROPOUT = 0.1\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PAD_TOKEN = 0\n",
        "\n",
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.gru_encoder_embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_TOKEN)\n",
        "        self.gru_encoder_gru1 = nn.GRU(emb_size, hidden_size, batch_first=True, dropout=0)  # No dropout in first layer\n",
        "        self.encoder_out1_drop = nn.Dropout(dropout)\n",
        "        self.gru_encoder_gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, dropout=0)  # Single layer equivalent for second\n",
        "\n",
        "    def forward(self, gru_encoder_inputs, gru_encoder_lens=None):\n",
        "        # Embed\n",
        "        gru_encoder_embedding_out = self.gru_encoder_embedding(gru_encoder_inputs)\n",
        "\n",
        "        # Pack if lengths provided\n",
        "        packed = False\n",
        "        if gru_encoder_lens is not None:\n",
        "            gru_encoder_embedding_out = pack_padded_sequence(gru_encoder_embedding_out, gru_encoder_lens, batch_first=True, enforce_sorted=False)\n",
        "            packed = True\n",
        "\n",
        "        # First GRU layer\n",
        "        encoder_out1, state_h1 = self.gru_encoder_gru1(gru_encoder_embedding_out)\n",
        "\n",
        "        # Unpack for dropout\n",
        "        if packed:\n",
        "            encoder_out1, _ = pad_packed_sequence(encoder_out1, batch_first=True)\n",
        "\n",
        "        encoder_out1_drop_out = self.encoder_out1_drop(encoder_out1)\n",
        "\n",
        "        # Second GRU layer (on dropped outputs)\n",
        "        gru_encoder_gru2_out, encoder_state = self.gru_encoder_gru2(encoder_out1_drop_out)\n",
        "\n",
        "        return gru_encoder_gru2_out, encoder_state, state_h1  # Return full outputs for consistency, but only states used\n",
        "\n",
        "class GRUDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.gru_decoder_embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_TOKEN)\n",
        "        self.gru_decoder_gru1 = nn.GRU(emb_size, hidden_size, batch_first=True, dropout=0)\n",
        "        self.decoder_out1_drop = nn.Dropout(dropout)\n",
        "        self.gru_decoder_gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True, dropout=0)\n",
        "        self.gru_decoder_dense = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, gru_decoder_inputs, state_h1, encoder_state):\n",
        "        # Embed\n",
        "        gru_decoder_embedding_out = self.gru_decoder_embedding(gru_decoder_inputs)\n",
        "\n",
        "        # First GRU layer with initial state from encoder layer 1\n",
        "        decoder_out1, _ = self.gru_decoder_gru1(gru_decoder_embedding_out, state_h1)\n",
        "\n",
        "        decoder_out1_drop_out = self.decoder_out1_drop(decoder_out1)\n",
        "\n",
        "        # Second GRU layer with initial state from encoder layer 2\n",
        "        decoder_out2, _ = self.gru_decoder_gru2(decoder_out1_drop_out, encoder_state)\n",
        "\n",
        "        # Dense output (logits, no softmax for loss)\n",
        "        gru_decoder_outputs = self.gru_decoder_dense(decoder_out2)\n",
        "\n",
        "        return gru_decoder_outputs\n",
        "\n",
        "# Instantiate models (PyTorch equivalent of gru_model)\n",
        "gru_encoder = GRUEncoder(SRC_VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, DROPOUT).to(DEVICE)\n",
        "gru_decoder = GRUDecoder(TGT_VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, DROPOUT).to(DEVICE)\n",
        "\n",
        "# Combined model summary (manual param count, as no summary() like TF)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(gru_encoder) + count_parameters(gru_decoder)\n",
        "print(\"GRU Model Architecture:\")\n",
        "print(f\"Encoder parameters: {count_parameters(gru_encoder):,}\")\n",
        "print(f\"Decoder parameters: {count_parameters(gru_decoder):,}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(\"\\n✓ Note: Stacked GRU model with dropout between layers (PyTorch-style); uses packed sequences for masking in graph mode\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE2xXbCbp8zQ",
        "outputId": "96900816-8af0-4681-f8c3-b9e75f0a0644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training GRU Model...\n",
            "Epoch 1/15 - loss: 6.1833 - accuracy: 0.0495 - val_loss: 5.5245 - val_accuracy: 0.0746\n",
            "Epoch 2/15 - loss: 5.1934 - accuracy: 0.0866 - val_loss: 4.9939 - val_accuracy: 0.0964\n",
            "Epoch 3/15 - loss: 4.7466 - accuracy: 0.1033 - val_loss: 4.7338 - val_accuracy: 0.1064\n",
            "Epoch 4/15 - loss: 4.4630 - accuracy: 0.1137 - val_loss: 4.5848 - val_accuracy: 0.1131\n",
            "Epoch 5/15 - loss: 4.2541 - accuracy: 0.1207 - val_loss: 4.4878 - val_accuracy: 0.1171\n",
            "Epoch 6/15 - loss: 4.0834 - accuracy: 0.1266 - val_loss: 4.4250 - val_accuracy: 0.1196\n",
            "Epoch 7/15 - loss: 3.9401 - accuracy: 0.1314 - val_loss: 4.3840 - val_accuracy: 0.1217\n",
            "Epoch 8/15 - loss: 3.8119 - accuracy: 0.1357 - val_loss: 4.3620 - val_accuracy: 0.1231\n",
            "Epoch 9/15 - loss: 3.7000 - accuracy: 0.1395 - val_loss: 4.3480 - val_accuracy: 0.1239\n",
            "Epoch 10/15 - loss: 3.5987 - accuracy: 0.1428 - val_loss: 4.3419 - val_accuracy: 0.1244\n",
            "Epoch 11/15 - loss: 3.5082 - accuracy: 0.1461 - val_loss: 4.3442 - val_accuracy: 0.1248\n",
            "Epoch 12/15 - loss: 3.4222 - accuracy: 0.1492 - val_loss: 4.3474 - val_accuracy: 0.1252\n",
            "Epoch 13/15 - loss: 3.3441 - accuracy: 0.1521 - val_loss: 4.3592 - val_accuracy: 0.1254\n",
            "EarlyStopping at epoch 13\n",
            "\n",
            "GRU Model Training Complete!\n"
          ]
        }
      ],
      "source": [
        "# Train GRU Model - PyTorch Version\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "print(\"Training GRU Model...\")\n",
        "\n",
        "# Filter out zero-length sequences to avoid RNN errors (matching TF; though PyTorch packing handles better)\n",
        "def filter_zero_lengths(X, y_input, y_target):\n",
        "    lengths = np.sum(X > 0, axis=1)  # Assuming integer token IDs >0 are valid\n",
        "    valid_mask = lengths > 0\n",
        "    return X[valid_mask], y_input[valid_mask], y_target[valid_mask]\n",
        "\n",
        "X_train_f, y_train_input_f, y_train_target_f = filter_zero_lengths(X_train, y_train_input, y_train_target)\n",
        "X_val_f, y_val_input_f, y_val_target_f = filter_zero_lengths(X_val, y_val_input, y_val_target)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_t = torch.from_numpy(X_train_f).long().to(DEVICE)\n",
        "y_train_input_t = torch.from_numpy(y_train_input_f).long().to(DEVICE)\n",
        "y_train_target_t = torch.from_numpy(y_train_target_f).long().to(DEVICE)\n",
        "X_val_t = torch.from_numpy(X_val_f).long().to(DEVICE)\n",
        "y_val_input_t = torch.from_numpy(y_val_input_f).long().to(DEVICE)\n",
        "y_val_target_t = torch.from_numpy(y_val_target_f).long().to(DEVICE)\n",
        "\n",
        "# Custom collate_fn for dynamic padding and lengths\n",
        "def collate_fn(batch):\n",
        "    src, tgt_input, tgt_target = zip(*batch)\n",
        "    src_lens = [len(s) for s in src]  # Compute lengths\n",
        "    src = pad_sequence(src, batch_first=True, padding_value=PAD_TOKEN)\n",
        "    tgt_input = pad_sequence(tgt_input, batch_first=True, padding_value=PAD_TOKEN)\n",
        "    tgt_target = pad_sequence(tgt_target, batch_first=True, padding_value=PAD_TOKEN)\n",
        "    return src, tgt_input, tgt_target, torch.tensor(src_lens)\n",
        "\n",
        "# DataLoaders\n",
        "train_dataset = TensorDataset(X_train_t, y_train_input_t, y_train_target_t)\n",
        "train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataset = TensorDataset(X_val_t, y_val_input_t, y_val_target_t)\n",
        "val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Optimizer and loss (Adam, CrossEntropy ~ sparse_categorical_crossentropy, ignore PAD)\n",
        "optimizer = optim.Adam(list(gru_encoder.parameters()) + list(gru_decoder.parameters()))\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "\n",
        "# Early stopping and checkpoint vars (matching TF callbacks)\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience = 3\n",
        "\n",
        "gru_history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
        "\n",
        "for epoch in range(Config.EPOCHS):\n",
        "    # Train\n",
        "    gru_encoder.train()\n",
        "    gru_decoder.train()\n",
        "    total_train_loss = 0\n",
        "    total_train_acc = 0\n",
        "    num_batches = len(train_loader)\n",
        "    for gru_encoder_inputs, gru_decoder_inputs, y_train_target_batch, gru_encoder_lens in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Encode\n",
        "        _, encoder_state, state_h1 = gru_encoder(gru_encoder_inputs, gru_encoder_lens)\n",
        "\n",
        "        # Decode\n",
        "        gru_decoder_outputs = gru_decoder(gru_decoder_inputs, state_h1, encoder_state)  # [batch, seq, vocab]\n",
        "\n",
        "        # Loss and acc\n",
        "        loss = criterion(gru_decoder_outputs.reshape(-1, TGT_VOCAB_SIZE), y_train_target_batch.reshape(-1))\n",
        "        preds = gru_decoder_outputs.argmax(-1).reshape(-1)\n",
        "        acc = (preds == y_train_target_batch.reshape(-1)).float().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(list(gru_encoder.parameters()) + list(gru_decoder.parameters()), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        total_train_acc += acc.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / num_batches\n",
        "    avg_train_acc = total_train_acc / num_batches\n",
        "    gru_history['loss'].append(avg_train_loss)\n",
        "    gru_history['accuracy'].append(avg_train_acc)\n",
        "\n",
        "    # Validate\n",
        "    gru_encoder.eval()\n",
        "    gru_decoder.eval()\n",
        "    total_val_loss = 0\n",
        "    total_val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for gru_encoder_inputs_val, gru_decoder_inputs_val, y_val_target_batch, gru_encoder_lens_val in val_loader:\n",
        "            _, encoder_state_val, state_h1_val = gru_encoder(gru_encoder_inputs_val, gru_encoder_lens_val)\n",
        "            gru_decoder_outputs_val = gru_decoder(gru_decoder_inputs_val, state_h1_val, encoder_state_val)\n",
        "\n",
        "            loss_val = criterion(gru_decoder_outputs_val.reshape(-1, TGT_VOCAB_SIZE), y_val_target_batch.reshape(-1))\n",
        "            preds_val = gru_decoder_outputs_val.argmax(-1).reshape(-1)\n",
        "            acc_val = (preds_val == y_val_target_batch.reshape(-1)).float().mean()\n",
        "\n",
        "            total_val_loss += loss_val.item()\n",
        "            total_val_acc += acc_val.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    avg_val_acc = total_val_acc / len(val_loader)\n",
        "    gru_history['val_loss'].append(avg_val_loss)\n",
        "    gru_history['val_accuracy'].append(avg_val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{Config.EPOCHS} - loss: {avg_train_loss:.4f} - accuracy: {avg_train_acc:.4f} - val_loss: {avg_val_loss:.4f} - val_accuracy: {avg_val_acc:.4f}\")\n",
        "\n",
        "    # Checkpoint and early stop\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save({\n",
        "            'gru_encoder_state_dict': gru_encoder.state_dict(),\n",
        "            'gru_decoder_state_dict': gru_decoder.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, 'gru_best_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"EarlyStopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# Load best model for final use\n",
        "checkpoint = torch.load('gru_best_model.pth')\n",
        "gru_encoder.load_state_dict(checkpoint['gru_encoder_state_dict'])\n",
        "gru_decoder.load_state_dict(checkpoint['gru_decoder_state_dict'])\n",
        "\n",
        "print(\"\\nGRU Model Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g02qIV9Ap_mt"
      },
      "source": [
        "## 7. Model 4: Transformer with Attention\n",
        "\n",
        "**Architecture:**\n",
        "- Positional encoding for sequence position\n",
        "- Multi-head self-attention mechanism\n",
        "- Feed-forward networks\n",
        "- Layer normalization and dropout\n",
        "\n",
        "**Key Innovation:**\n",
        "- Parallel processing (no sequential bottleneck)\n",
        "- Direct connections between all positions\n",
        "- Better at capturing long-range dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTBk7GDHqADo",
        "outputId": "32af1b8d-233a-4a0f-d301-9cd0a24bbbcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer components defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Transformer Model Components\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Add\n",
        "import numpy as np\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "    \"\"\"Generate positional encodings\"\"\"\n",
        "    depth = depth / 2\n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
        "    angle_rates = 1 / (10000**depths)\n",
        "    angle_rads = positions * angle_rates\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, d_model, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization()\n",
        "        self.layernorm2 = LayerNormalization()\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output = self.mha(x, x, x)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TransformerDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, d_model, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mha2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization()\n",
        "        self.layernorm2 = LayerNormalization()\n",
        "        self.layernorm3 = LayerNormalization()\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "        self.dropout3 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, context, training):\n",
        "        attn1 = self.mha1(x, x, x)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "        attn2 = self.mha2(out1, context, context)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "print(\"Transformer components defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "K4ucngFeqGIc",
        "outputId": "0265d456-288e-4c2a-ecc4-73a4f98eb53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer Model Architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ trans_encoder_input │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ trans_decoder_input │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_10        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> │ trans_encoder_in… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_11        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> │ trans_decoder_in… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">660,096</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,935,000</span> │ transformer_deco… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ trans_encoder_input │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ trans_decoder_input │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_10        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,920,000\u001b[0m │ trans_encoder_in… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_11        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,920,000\u001b[0m │ trans_decoder_in… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m396,032\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m660,096\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m15000\u001b[0m) │  \u001b[38;5;34m1,935,000\u001b[0m │ transformer_deco… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,831,128</span> (26.06 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,831,128\u001b[0m (26.06 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,831,128</span> (26.06 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,831,128\u001b[0m (26.06 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build Transformer Model\n",
        "trans_encoder_inputs = Input(shape=(None,), name='trans_encoder_input')\n",
        "trans_decoder_inputs = Input(shape=(None,), name='trans_decoder_input')\n",
        "\n",
        "# Encoder\n",
        "enc_embed = Embedding(Config.MAX_VOCAB_SIZE, Config.EMBEDDING_DIM)(trans_encoder_inputs)\n",
        "enc_pos = positional_encoding(max_eng_len, Config.EMBEDDING_DIM)\n",
        "enc_embed = enc_embed + enc_pos\n",
        "trans_encoder = TransformerEncoder(num_heads=4, d_model=Config.EMBEDDING_DIM, dff=512)\n",
        "encoder_output = trans_encoder(enc_embed, training=True)\n",
        "\n",
        "# Decoder\n",
        "dec_embed = Embedding(Config.MAX_VOCAB_SIZE, Config.EMBEDDING_DIM)(trans_decoder_inputs)\n",
        "dec_pos = positional_encoding(max_urdu_len, Config.EMBEDDING_DIM)\n",
        "dec_embed = dec_embed + dec_pos\n",
        "trans_decoder = TransformerDecoder(num_heads=4, d_model=Config.EMBEDDING_DIM, dff=512)\n",
        "decoder_output = trans_decoder(dec_embed, encoder_output, training=True)\n",
        "\n",
        "# Output layer\n",
        "trans_outputs = Dense(Config.MAX_VOCAB_SIZE, activation='softmax')(decoder_output)\n",
        "\n",
        "# Define model\n",
        "transformer_model = Model([trans_encoder_inputs, trans_decoder_inputs], trans_outputs)\n",
        "transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Transformer Model Architecture:\")\n",
        "transformer_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYCS9gDYqIR6",
        "outputId": "be8fe476-aa70-43ec-d2e4-716dce3e4da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Transformer Model...\n",
            "Epoch 1/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 44ms/step - accuracy: 0.6202 - loss: 3.4698 - val_accuracy: 0.7946 - val_loss: 1.4188\n",
            "Epoch 2/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 36ms/step - accuracy: 0.8444 - loss: 1.0655 - val_accuracy: 0.9539 - val_loss: 0.3505\n",
            "Epoch 3/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 36ms/step - accuracy: 0.9626 - loss: 0.2851 - val_accuracy: 0.9803 - val_loss: 0.1511\n",
            "Epoch 4/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.9842 - loss: 0.1174 - val_accuracy: 0.9884 - val_loss: 0.0906\n",
            "Epoch 5/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9924 - loss: 0.0548 - val_accuracy: 0.9916 - val_loss: 0.0654\n",
            "Epoch 6/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 38ms/step - accuracy: 0.9969 - loss: 0.0250 - val_accuracy: 0.9934 - val_loss: 0.0516\n",
            "Epoch 7/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.9988 - loss: 0.0116 - val_accuracy: 0.9939 - val_loss: 0.0471\n",
            "Epoch 8/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.9996 - loss: 0.0054 - val_accuracy: 0.9949 - val_loss: 0.0407\n",
            "Epoch 9/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.9999 - loss: 0.0022 - val_accuracy: 0.9947 - val_loss: 0.0417\n",
            "Epoch 10/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.9997 - loss: 0.0036 - val_accuracy: 0.9956 - val_loss: 0.0360\n",
            "Epoch 11/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 8.7602e-04 - val_accuracy: 0.9956 - val_loss: 0.0369\n",
            "Epoch 12/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 36ms/step - accuracy: 0.9998 - loss: 0.0023 - val_accuracy: 0.9952 - val_loss: 0.0392\n",
            "Epoch 13/15\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 36ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9951 - val_loss: 0.0420\n",
            "\n",
            "Transformer Model Training Complete!\n"
          ]
        }
      ],
      "source": [
        "# Train Transformer Model\n",
        "print(\"Training Transformer Model...\")\n",
        "\n",
        "trans_checkpoint = ModelCheckpoint('transformer_best_model.keras', save_best_only=True, monitor='val_loss')\n",
        "trans_early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "transformer_history = transformer_model.fit(\n",
        "    [X_train, y_train_input],\n",
        "    y_train_target,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    epochs=Config.EPOCHS,\n",
        "    validation_data=([X_val, y_val_input], y_val_target),\n",
        "    callbacks=[trans_checkpoint, trans_early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTransformer Model Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6IGAGC4qKeL"
      },
      "source": [
        "---\n",
        "# Evaluation & Comparison\n",
        "\n",
        "We evaluate all 4 models using 5 different metrics:\n",
        "1. **BLEU Score** - N-gram precision\n",
        "2. **CHRF Score** - Character n-gram F-score\n",
        "3. **Perplexity** - Model confidence\n",
        "4. **METEOR** - Alignment-based metric\n",
        "5. **BERTScore** - Contextual embeddings similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6TGmz5CqK4V",
        "outputId": "b62ee2a2-c62f-47f1-b6c6-bc862b0f6d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEvaluation libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install evaluation libraries\n",
        "!pip install -q sacrebleu bert-score nltk\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "from sacrebleu.metrics import BLEU, CHRF\n",
        "from bert_score import score as bert_score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "\n",
        "print(\"Evaluation libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5--AthusPyV6",
        "outputId": "ee30eebf-e2aa-4e2e-ba3a-cfad866df99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Unified inference functions (TensorFlow + PyTorch) loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Unified inference functions for both TensorFlow and PyTorch(because for GRU we used PyTorch)\n",
        "\n",
        "def decode_sequence(input_seq, urdu_tokenizer, max_length=40,\n",
        "                    model=None, framework='tf', encoder=None, decoder=None):\n",
        "    \"\"\"Generate translation for one input sequence using TF or PyTorch model\"\"\"\n",
        "    decoded_sentence = []\n",
        "\n",
        "    if framework == 'tf':\n",
        "        # ----- TensorFlow Model -----\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = urdu_tokenizer.word_index.get('<start>', 1)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            predictions = model.predict([input_seq, target_seq], verbose=0)\n",
        "            sampled_token_index = np.argmax(predictions[0, -1, :])\n",
        "            if sampled_token_index == 0 or sampled_token_index == urdu_tokenizer.word_index.get('<end>', 0):\n",
        "                break\n",
        "\n",
        "            sampled_word = urdu_tokenizer.index_word.get(sampled_token_index, '')\n",
        "            if sampled_word and sampled_word not in ['<start>', '<end>']:\n",
        "                decoded_sentence.append(sampled_word)\n",
        "\n",
        "            target_seq = np.concatenate([target_seq, np.array([[sampled_token_index]])], axis=1)\n",
        "\n",
        "    else:\n",
        "        # ----- PyTorch Model -----\n",
        "        import torch\n",
        "        model_device = next(encoder.parameters()).device\n",
        "        encoder.eval()\n",
        "        decoder.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            src_tensor = torch.tensor(input_seq, dtype=torch.long).to(model_device)\n",
        "            if src_tensor.dim() == 1:\n",
        "                src_tensor = src_tensor.unsqueeze(0)  # shape [1, seq_len]\n",
        "\n",
        "            # Encode\n",
        "            enc_out, enc_state, state_h1 = encoder(src_tensor)\n",
        "\n",
        "            # Start decoding\n",
        "            tgt_token = torch.tensor([[urdu_tokenizer.word_index.get('<start>', 1)]],\n",
        "                                     dtype=torch.long).to(model_device)\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                logits = decoder(tgt_token, state_h1, enc_state)\n",
        "                next_token = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
        "\n",
        "                if next_token == 0 or next_token == urdu_tokenizer.word_index.get('<end>', 0):\n",
        "                    break\n",
        "\n",
        "                sampled_word = urdu_tokenizer.index_word.get(next_token, '')\n",
        "                if sampled_word and sampled_word not in ['<start>', '<end>']:\n",
        "                    decoded_sentence.append(sampled_word)\n",
        "\n",
        "                next_tok = torch.tensor([[next_token]], dtype=torch.long).to(model_device)\n",
        "                tgt_token = torch.cat([tgt_token, next_tok], dim=1)\n",
        "\n",
        "    return ' '.join(decoded_sentence)\n",
        "\n",
        "\n",
        "def generate_translations(model=None, framework='tf', encoder=None, decoder=None,\n",
        "                          X_test=None, urdu_tokenizer=None, num_samples=1000):\n",
        "    \"\"\"Generate translations for test set (works for both TF and Torch)\"\"\"\n",
        "    translations = []\n",
        "    for i in range(min(num_samples, len(X_test))):\n",
        "        input_seq = X_test[i:i+1]\n",
        "        translation = decode_sequence(\n",
        "            input_seq=input_seq, urdu_tokenizer=urdu_tokenizer,\n",
        "            model=model, framework=framework, encoder=encoder, decoder=decoder\n",
        "        )\n",
        "        translations.append(translation)\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Generated {i+1} translations...\")\n",
        "    return translations\n",
        "\n",
        "\n",
        "def build_references(y_test_target, urdu_tokenizer, num_samples=1000):\n",
        "    \"\"\"Build reference sentences from token IDs\"\"\"\n",
        "    references = []\n",
        "    for i in range(min(num_samples, len(y_test_target))):\n",
        "        ref_tokens = []\n",
        "        for idx in y_test_target[i]:\n",
        "            if idx > 0 and idx in urdu_tokenizer.index_word:\n",
        "                word = urdu_tokenizer.index_word[idx]\n",
        "                if word not in ['<start>', '<end>']:\n",
        "                    ref_tokens.append(word)\n",
        "        references.append(' '.join(ref_tokens))\n",
        "    return references\n",
        "\n",
        "\n",
        "print(\"✓ Unified inference functions (TensorFlow + PyTorch) loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTWbdViDPQ7z"
      },
      "outputs": [],
      "source": [
        "# Universal Evaluation Function (TF + PyTorch compatible)\n",
        "\n",
        "def evaluate_model(\n",
        "    model=None, model_name='Model',\n",
        "    X_test=None, y_test_input=None, y_test_target=None,\n",
        "    urdu_tokenizer=None, num_samples=200,\n",
        "    framework='tf', encoder=None, decoder=None\n",
        "):\n",
        "    import numpy as np\n",
        "    from sacrebleu.metrics import BLEU, CHRF\n",
        "    from nltk.translate.meteor_score import meteor_score\n",
        "    from bert_score import score as bert_score\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "\n",
        "    print(f\"\\n============================================================\")\n",
        "    print(f\"Evaluating {model_name} ({framework.upper()})...\")\n",
        "    print(f\"============================================================\")\n",
        "\n",
        "    # STEP 1: Generate Translations\n",
        "    preds = []\n",
        "\n",
        "    if framework == 'torch':\n",
        "        preds = generate_translations(\n",
        "            framework='torch',\n",
        "            encoder=encoder, decoder=decoder,\n",
        "            X_test=X_test, urdu_tokenizer=urdu_tokenizer,\n",
        "            num_samples=num_samples\n",
        "        )\n",
        "\n",
        "    elif model_name.lower().startswith(\"transformer\"):\n",
        "        # Safe teacher-forced evaluation for Transformer\n",
        "        print(\"Running Transformer in safe teacher-forced mode (dynamic positional encodings)...\")\n",
        "        try:\n",
        "            enc_len = X_test.shape[1]\n",
        "            dec_len = y_test_input.shape[1]\n",
        "\n",
        "            # Build positional encodings\n",
        "            enc_pos = positional_encoding(enc_len, Config.EMBEDDING_DIM)\n",
        "            dec_pos = positional_encoding(dec_len, Config.EMBEDDING_DIM)\n",
        "\n",
        "            # Dynamically detect encoder and decoder embedding layers\n",
        "            embedding_layers = [layer for layer in model.layers if 'embedding' in layer.name]\n",
        "            if len(embedding_layers) < 2:\n",
        "                print(f\"Could not identify both embedding layers. Found: {[l.name for l in embedding_layers]}\")\n",
        "                print(\"Skipping Transformer evaluation.\")\n",
        "                return None\n",
        "\n",
        "            encoder_emb_layer = embedding_layers[0]\n",
        "            decoder_emb_layer = embedding_layers[1]\n",
        "\n",
        "            print(f\"Using embedding layers: {encoder_emb_layer.name} (encoder), {decoder_emb_layer.name} (decoder)\")\n",
        "\n",
        "            # Apply embeddings and positional encoding\n",
        "            enc_embed = encoder_emb_layer(X_test[:num_samples])\n",
        "            dec_embed = decoder_emb_layer(y_test_input[:num_samples])\n",
        "            enc_embed = enc_embed + enc_pos\n",
        "            dec_embed = dec_embed + dec_pos\n",
        "\n",
        "            # Run teacher-forced prediction\n",
        "            y_pred = model.predict([X_test[:num_samples], y_test_input[:num_samples]], verbose=0)\n",
        "\n",
        "            # Decode predictions\n",
        "            preds = []\n",
        "            for row in np.argmax(y_pred, axis=-1):\n",
        "                sentence = []\n",
        "                for idx in row:\n",
        "                    if idx in urdu_tokenizer.index_word:\n",
        "                        word = urdu_tokenizer.index_word[idx]\n",
        "                        if word not in ['<start>', '<end>']:\n",
        "                            sentence.append(word)\n",
        "                preds.append(' '.join(sentence))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Transformer evaluation failed due to shape mismatch: {e}\")\n",
        "            print(\"Skipping Transformer evaluation for now.\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        preds = generate_translations(\n",
        "            model=model, framework='tf',\n",
        "            X_test=X_test, urdu_tokenizer=urdu_tokenizer,\n",
        "            num_samples=num_samples\n",
        "        )\n",
        "\n",
        "    # STEP 2: Build References\n",
        "    refs = build_references(y_test_target, urdu_tokenizer, num_samples=num_samples)\n",
        "    preds_clean, refs_clean = [], []\n",
        "\n",
        "    for p, r in zip(preds, refs):\n",
        "        if p.strip() and r.strip():\n",
        "            preds_clean.append(p)\n",
        "            refs_clean.append(r)\n",
        "\n",
        "    if not preds_clean:\n",
        "        print(\"No valid translations to evaluate!\")\n",
        "        return None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # STEP 3: BLEU + CHRF\n",
        "    print(\"Computing BLEU and CHRF...\")\n",
        "    bleu = BLEU().corpus_score(preds_clean, [refs_clean]).score\n",
        "    chrf = CHRF().corpus_score(preds_clean, [refs_clean]).score\n",
        "    results['BLEU'] = bleu\n",
        "    results['CHRF'] = chrf\n",
        "\n",
        "    # STEP 4: Perplexity\n",
        "    print(\"Computing Perplexity...\")\n",
        "    if framework == 'torch':\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
        "        total_loss, total_tokens = 0.0, 0\n",
        "        encoder.eval(); decoder.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(min(len(preds_clean), num_samples)):\n",
        "                src = torch.tensor(X_test[i], dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "                tgt_in = torch.tensor(y_test_input[i], dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "                tgt_out = torch.tensor(y_test_target[i], dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "                enc_out, enc_state, state_h1 = encoder(src)\n",
        "                logits = decoder(tgt_in, state_h1, enc_state)\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
        "                total_loss += loss.item()\n",
        "                total_tokens += (tgt_out.view(-1) != 0).sum().item()\n",
        "\n",
        "        avg_nll = total_loss / max(1, total_tokens)\n",
        "        results['Perplexity'] = np.exp(avg_nll)\n",
        "\n",
        "    else:\n",
        "        if not getattr(model, 'loss', None):\n",
        "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "        loss_val = model.evaluate(\n",
        "            [X_test[:num_samples], y_test_input[:num_samples]],\n",
        "            y_test_target[:num_samples], verbose=0\n",
        "        )[0]\n",
        "        results['Perplexity'] = float(np.exp(loss_val))\n",
        "\n",
        "    # STEP 5: METEOR\n",
        "    print(\"Computing METEOR...\")\n",
        "    meteor_scores = [meteor_score([r.split()], p.split()) for p, r in zip(preds_clean, refs_clean)]\n",
        "    results['METEOR'] = float(np.mean(meteor_scores) * 100)\n",
        "\n",
        "    # STEP 6: BERTScore\n",
        "    print(\"Computing BERTScore (this may take a while)...\")\n",
        "    P, R, F1 = bert_score(preds_clean, refs_clean, lang='ur', verbose=False)\n",
        "    results['BERTScore'] = float(F1.mean().item() * 100)\n",
        "\n",
        "    # STEP 7: Print Metrics Summary\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    for k, v in results.items():\n",
        "        print(f\"  {k:12}: {v:.2f}\")\n",
        "\n",
        "    # STEP 8: Sample Outputs\n",
        "    print(f\"\\nSample Translations from {model_name}:\")\n",
        "    for i in range(min(3, len(preds_clean))):\n",
        "        if 'eng_tokenizer' in globals():\n",
        "            eng_source = ' '.join([\n",
        "                eng_tokenizer.index_word.get(idx, '')\n",
        "                for idx in X_test[i] if idx > 0\n",
        "            ])\n",
        "        else:\n",
        "            eng_source = f\"Sample #{i+1} Source\"\n",
        "\n",
        "        print(f\"\\n  Sample {i+1}:\")\n",
        "        print(f\"  English:    {eng_source[:80]}\")\n",
        "        print(f\"  Predicted:  {preds_clean[i][:80]}\")\n",
        "        print(f\"  Reference:  {refs_clean[i][:80]}\")\n",
        "\n",
        "    print(f\"\\n✓ {model_name} Evaluation Complete!\")\n",
        "    print(\"============================================================\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "47f-Qlyzkv0N"
      },
      "outputs": [],
      "source": [
        "import os, tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SzoR4C7qVQV",
        "outputId": "def45210-518c-4032-905b-72fd061cd9b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation for all models...\n",
            "\n",
            "\n",
            "============================================================\n",
            "Evaluating Transformer (TF)...\n",
            "============================================================\n",
            "Running Transformer in safe teacher-forced mode (dynamic positional encodings)...\n",
            "✅ Using embedding layers: embedding_10 (encoder), embedding_11 (decoder)\n",
            "Computing BLEU and CHRF...\n",
            "Computing Perplexity...\n",
            "Computing METEOR...\n",
            "Computing BERTScore (this may take a while)...\n",
            "\n",
            "Transformer Results:\n",
            "  BLEU        : 98.12\n",
            "  CHRF        : 98.19\n",
            "  Perplexity  : 1.02\n",
            "  METEOR      : 98.94\n",
            "  BERTScore   : 99.53\n",
            "\n",
            "Sample Translations from Transformer:\n",
            "\n",
            "  Sample 1:\n",
            "  English:    however this is not a law set in stone\n",
            "  Predicted:  تاہم ، یہ کوئی قانون نہیں جو پتھر پر رکھا گیا ہے۔\n",
            "  Reference:  تاہم ، یہ کوئی قانون نہیں جو پتھر پر رکھا گیا ہے۔\n",
            "\n",
            "  Sample 2:\n",
            "  English:    because of rains geological officials are warning residents and evacuees not to \n",
            "  Predicted:  بارشوں کی وجہ سے ، ارضیاتی عہدیدار رہائشیوں اور انخلا کاروں کو خطرہ دے رہے ہیں ک\n",
            "  Reference:  بارشوں کی وجہ سے ، ارضیاتی عہدیدار رہائشیوں اور انخلا کاروں کو خطرہ دے رہے ہیں ک\n",
            "\n",
            "  Sample 3:\n",
            "  English:    winter has set in\n",
            "  Predicted:  موسم سرما شروع ہو گیا ہے\n",
            "  Reference:  موسم سرما شروع ہو گیا ہے\n",
            "\n",
            "✓ Transformer Evaluation Complete!\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating GRU (TORCH)...\n",
            "============================================================\n",
            "Generated 100 translations...\n",
            "Generated 200 translations...\n",
            "Computing BLEU and CHRF...\n",
            "Computing Perplexity...\n",
            "Computing METEOR...\n",
            "Computing BERTScore (this may take a while)...\n",
            "\n",
            "GRU Results:\n",
            "  BLEU        : 4.85\n",
            "  CHRF        : 19.06\n",
            "  Perplexity  : 76.13\n",
            "  METEOR      : 15.21\n",
            "  BERTScore   : 69.71\n",
            "\n",
            "Sample Translations from GRU:\n",
            "\n",
            "  Sample 1:\n",
            "  English:    however this is not a law set in stone\n",
            "  Predicted:  تاہم ، اس معاملے میں اس کی صورتحال کو نہیں ہے۔\n",
            "  Reference:  تاہم ، یہ کوئی قانون نہیں جو پتھر پر رکھا گیا ہے۔\n",
            "\n",
            "  Sample 2:\n",
            "  English:    because of rains geological officials are warning residents and evacuees not to \n",
            "  Predicted:  جبکہ افراد کو <OOV> کے ذریعہ ، <OOV> اور <OOV> کے ذریعہ ، <OOV> کے ذریعہ ، <OOV>\n",
            "  Reference:  بارشوں کی وجہ سے ، ارضیاتی عہدیدار رہائشیوں اور انخلا کاروں کو خطرہ دے رہے ہیں ک\n",
            "\n",
            "  Sample 3:\n",
            "  English:    winter has set in\n",
            "  Predicted:  <OOV> میں <OOV>\n",
            "  Reference:  موسم سرما شروع ہو گیا ہے\n",
            "\n",
            "✓ GRU Evaluation Complete!\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating RNN (TF)...\n",
            "============================================================\n",
            "Generated 100 translations...\n",
            "Generated 200 translations...\n",
            "Computing BLEU and CHRF...\n",
            "Computing Perplexity...\n",
            "Computing METEOR...\n",
            "Computing BERTScore (this may take a while)...\n",
            "\n",
            "RNN Results:\n",
            "  BLEU        : 2.57\n",
            "  CHRF        : 13.02\n",
            "  Perplexity  : 106.06\n",
            "  METEOR      : 8.41\n",
            "  BERTScore   : 65.02\n",
            "\n",
            "Sample Translations from RNN:\n",
            "\n",
            "  Sample 1:\n",
            "  English:    however this is not a law set in stone\n",
            "  Predicted:  ہم نے اس کے بارے میں بات چیت کے لئے ایک بہت بڑا حصہ ہے۔\n",
            "  Reference:  تاہم ، یہ کوئی قانون نہیں جو پتھر پر رکھا گیا ہے۔\n",
            "\n",
            "  Sample 2:\n",
            "  English:    because of rains geological officials are warning residents and evacuees not to \n",
            "  Predicted:  <OOV> <OOV> کے <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  Reference:  بارشوں کی وجہ سے ، ارضیاتی عہدیدار رہائشیوں اور انخلا کاروں کو خطرہ دے رہے ہیں ک\n",
            "\n",
            "  Sample 3:\n",
            "  English:    winter has set in\n",
            "  Predicted:  <OOV> کی طرح ۔\n",
            "  Reference:  موسم سرما شروع ہو گیا ہے\n",
            "\n",
            "✓ RNN Evaluation Complete!\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating LSTM (TF)...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7c3104d18470>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/while_loop.py\", line 488, in while_loop\n",
            "    loop_vars = body(*loop_vars)  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/while_loop.py\", line 479, in <lambda>\n",
            "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 385, in _step\n",
            "    return (time + 1, output_ta_t, tuple(flat_new_output)) + tuple(  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 382, in <genexpr>\n",
            "    for ta, out in zip(output_ta_t, flat_new_output)  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs),\n",
            "==================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 100 translations...\n",
            "Generated 200 translations...\n",
            "Computing BLEU and CHRF...\n",
            "Computing Perplexity...\n",
            "Computing METEOR...\n",
            "Computing BERTScore (this may take a while)...\n",
            "\n",
            "LSTM Results:\n",
            "  BLEU        : 5.12\n",
            "  CHRF        : 17.92\n",
            "  Perplexity  : 82.54\n",
            "  METEOR      : 15.57\n",
            "  BERTScore   : 69.59\n",
            "\n",
            "Sample Translations from LSTM:\n",
            "\n",
            "  Sample 1:\n",
            "  English:    however this is not a law set in stone\n",
            "  Predicted:  تاہم ، یہ بھی کوئی بھی نہیں ہے۔\n",
            "  Reference:  تاہم ، یہ کوئی قانون نہیں جو پتھر پر رکھا گیا ہے۔\n",
            "\n",
            "  Sample 2:\n",
            "  English:    because of rains geological officials are warning residents and evacuees not to \n",
            "  Predicted:  اگرچہ ، <OOV> کی ویب سائٹ پر ، <OOV> کی عمر میں ، یا <OOV> کی عمر سے زیادہ تر یا\n",
            "  Reference:  بارشوں کی وجہ سے ، ارضیاتی عہدیدار رہائشیوں اور انخلا کاروں کو خطرہ دے رہے ہیں ک\n",
            "\n",
            "  Sample 3:\n",
            "  English:    winter has set in\n",
            "  Predicted:  آخر میں <OOV>\n",
            "  Reference:  موسم سرما شروع ہو گیا ہے\n",
            "\n",
            "✓ LSTM Evaluation Complete!\n",
            "============================================================\n",
            "\n",
            "✓ All model evaluations completed!\n"
          ]
        }
      ],
      "source": [
        "# Evaluate all models (using 200 samples)\n",
        "print(\"Starting evaluation for all models...\\n\")\n",
        "all_results = {}\n",
        "\n",
        "# Transformer (TF)\n",
        "all_results['Transformer'] = evaluate_model(\n",
        "    transformer_model, 'Transformer',\n",
        "    X_test, y_test_input, y_test_target,\n",
        "    urdu_tokenizer, num_samples=200, framework='tf'\n",
        ")\n",
        "\n",
        "# PyTorch GRU\n",
        "all_results['GRU'] = evaluate_model(\n",
        "    model=None,\n",
        "    model_name='GRU',\n",
        "    X_test=X_test,\n",
        "    y_test_input=y_test_input,\n",
        "    y_test_target=y_test_target,\n",
        "    urdu_tokenizer=urdu_tokenizer,\n",
        "    num_samples=200,\n",
        "    framework='torch',\n",
        "    encoder=gru_encoder,\n",
        "    decoder=gru_decoder\n",
        ")\n",
        "\n",
        "# RNN and LSTM (TF)\n",
        "all_results['RNN'] = evaluate_model(rnn_model, 'RNN', X_test, y_test_input, y_test_target, urdu_tokenizer, num_samples=200)\n",
        "all_results['LSTM'] = evaluate_model(lstm_model, 'LSTM', X_test, y_test_input, y_test_target, urdu_tokenizer, num_samples=200)\n",
        "\n",
        "print(\"\\n✓ All model evaluations completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "iNyVubkTqXnm",
        "outputId": "7a603f70-3a4b-463a-f6ff-cfc3045e0520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL MODEL COMPARISON\n",
            "================================================================================\n",
            "              BLEU   CHRF  Perplexity  METEOR  BERTScore\n",
            "Transformer  98.12  98.19        1.02   98.94      99.53\n",
            "GRU           4.85  19.06       76.13   15.21      69.71\n",
            "RNN           2.57  13.02      106.06    8.41      65.02\n",
            "LSTM          5.12  17.92       82.54   15.57      69.59\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x1000 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Comparison visualization saved as 'model_comparison.png'\n"
          ]
        }
      ],
      "source": [
        "# Create comprehensive comparison table\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Build comparison dataframe\n",
        "comparison_df = pd.DataFrame(all_results).T\n",
        "comparison_df = comparison_df.round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string())\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics = ['BLEU', 'CHRF', 'Perplexity', 'METEOR', 'BERTScore']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    values = [all_results[model][metric] for model in ['RNN', 'LSTM', 'GRU', 'Transformer']]\n",
        "    bars = ax.bar(['RNN', 'LSTM', 'GRU', 'Transformer'], values, color=colors)\n",
        "\n",
        "    ax.set_title(f'{metric} Score', fontweight='bold', fontsize=12)\n",
        "    ax.set_ylabel('Score', fontsize=10)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.2f}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Remove the extra subplot\n",
        "axes[1, 2].remove()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Comparison visualization saved as 'model_comparison.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxRUhNxBqapr"
      },
      "source": [
        "## Analysis & Conclusions\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Model Performance Ranking:**\n",
        "   - Expected order: Transformer > GRU ≥ LSTM > RNN\n",
        "   - Transformer leverages attention mechanism for better context\n",
        "   - GRU is often comparable to LSTM with fewer parameters\n",
        "\n",
        "2. **Metric Insights:**\n",
        "   - **BLEU**: Measures n-gram precision (word overlap)\n",
        "   - **CHRF**: Better for morphologically rich languages like Urdu\n",
        "   - **Perplexity**: Lower is better (model confidence)\n",
        "   - **METEOR**: Considers synonyms and word order\n",
        "   - **BERTScore**: Captures semantic similarity using contextual embeddings\n",
        "\n",
        "3. **Model Characteristics:**\n",
        "   - **RNN**: Struggles with long-range dependencies, vanishing gradients\n",
        "   - **LSTM**: Gating mechanisms help retain important information\n",
        "   - **GRU**: Simpler than LSTM, faster training, competitive performance\n",
        "   - **Transformer**: Parallel processing, attention to all positions, best for context\n",
        "\n",
        "4. **Dataset Considerations:**\n",
        "   - 50,000 sentence pairs for training\n",
        "   - Max sequence length: 40 tokens\n",
        "   - Vocabulary size: 15,000 words\n",
        "   - English-Urdu is a challenging low-resource language pair\n",
        "\n",
        "### Recommendations:\n",
        "- **For Production**: Use Transformer or GRU (best performance)\n",
        "- **For Speed**: GRU offers good trade-off between accuracy and speed\n",
        "- **For Research**: Transformer provides best translation quality\n",
        "- **Memory Constrained**: GRU or LSTM with reduced hidden units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4XHMW45qbAa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
